{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254b498c",
   "metadata": {},
   "source": [
    "# Logit Regression Model for Features Importance: `/r/PC` and `/r/PCM`\n",
    "- This notebook is used to create the processed data for 3.1-notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5e6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from matplotlib import rc\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "sys.path += ['../']\n",
    "\n",
    "from config import processed_data_path, raw_data_path, figure_path\n",
    "\n",
    "# Parameters\n",
    "DATA_PATH = raw_data_path\n",
    "OUTPUT_PATH = processed_data_path\n",
    "FIGURES_PATH = figure_path\n",
    "\n",
    "SUBREDDITS = {\n",
    "    'PoliticalCompass': 'PC',\n",
    "    'PoliticalCompassMemes': 'PCM'}\n",
    "\n",
    "POPULARITY_BIN = ['s1', 's2', 's3', 's4']\n",
    "\n",
    "FLAIRS2SOCIAL = {\n",
    "    'AuthLeft': 'Auth',\n",
    "    'AuthCenter': 'Auth',\n",
    "    'AuthRight': 'Auth',\n",
    "    'Left': 'Centrist_social',#\n",
    "    'Centrist': 'Centrist_social',#\n",
    "    'Right': 'Centrist_social',#\n",
    "    'LibLeft': 'Lib',\n",
    "    'LibCenter': 'Lib',\n",
    "    'LibRight': 'Lib'\n",
    "}\n",
    "FLAIRS2ECONOMIC = {\n",
    "    'AuthLeft': 'Left',\n",
    "    'Left': 'Left',\n",
    "    'LibLeft': 'Left',\n",
    "    'AuthCenter': 'Centrist_economic',#\n",
    "    'Centrist': 'Centrist_economic',#\n",
    "    'LibCenter': 'Centrist_economic',#\n",
    "    'LibRight': 'Right',\n",
    "    'Right': 'Right',\n",
    "    'AuthRight': 'Right'\n",
    "}\n",
    "SOCIO_DEM2CLASSES = {\n",
    "    'age': ['Young', 'Old'],    \n",
    "    'gender': ['Male', 'Female'],   \n",
    "    'affluence': ['Poor', 'Rich']\n",
    "}\n",
    "SOCIO_DEM_CLASSES = [x for pair in SOCIO_DEM2CLASSES.values() for x in pair]\n",
    "N_SOCIO = len(SOCIO_DEM_CLASSES)\n",
    "\n",
    "IDEOLOGIES2CLASSES = {\n",
    "    'economic': ['Left','Right'],\n",
    "    'social': ['Lib', 'Auth']\n",
    "}\n",
    "\n",
    "IDEOLOGIES_CLASSES = [x for pair in IDEOLOGIES2CLASSES.values() for x in pair]\n",
    "N_IDEOLOGIES = len(IDEOLOGIES_CLASSES)\n",
    "\n",
    "CLASSES = np.concatenate([SOCIO_DEM_CLASSES,\n",
    "                          IDEOLOGIES_CLASSES,\n",
    "                          POPULARITY_BIN\n",
    "                         ])\n",
    "N = len(CLASSES)\n",
    "\n",
    "# Functions\n",
    "def ideologic_features(edges: pd.DataFrame, users: pd.DataFrame) -> pd.DataFrame:\n",
    "    # edges: child, parent\n",
    "    # users: author, flair\n",
    "    if ('child' in set(edges.keys())) & ('parent' in set(edges.keys())) & \\\n",
    "    ('author' in set(users.keys())) & ('flair' in set(users.keys())):\n",
    "        df = (edges\n",
    "              .join(users.set_index('author'), on='child')\n",
    "              .rename(columns={'flair': 'social_child'})\n",
    "              .join(users.set_index('author'), on='child')\n",
    "              .rename(columns={'flair': 'economic_child'})\n",
    "              .join(users.set_index('author'), on='parent')\n",
    "              .rename(columns={'flair': 'social_parent'})\n",
    "              .join(users.set_index('author'), on='parent')\n",
    "              .rename(columns={'flair': 'economic_parent'})\n",
    "             )\n",
    "\n",
    "        df['social_child'] = df['social_child'].map(FLAIRS2SOCIAL.get).values\n",
    "        df['economic_child'] = df['economic_child'].map(FLAIRS2ECONOMIC.get).values\n",
    "\n",
    "        df['social_parent'] = df['social_parent'].map(FLAIRS2SOCIAL.get).values\n",
    "        df['economic_parent'] = df['economic_parent'].map(FLAIRS2ECONOMIC.get).values\n",
    "        \n",
    "        return df\n",
    "        # (return) edges: child, parent, Social_child, Economic_social, Social_parent, Economic_parent\n",
    "    else:\n",
    "        return pd.DataFrame({})\n",
    "\n",
    "def quantile_normalize(data):\n",
    "    return sklearn.preprocessing.quantile_transform(\n",
    "            data.values.reshape(-1, 1), copy=True)\n",
    "\n",
    "def subsample_neg_pairs(graph_df):\n",
    "    \"\"\"\n",
    "    Random Network (Null Model) as a directed, weighted, random network (RN).\n",
    "    This Network is obtained by reshuffling links of the original network while preserving\n",
    "    the in- and out-strength (weighted degree) of each node.\n",
    "    Args:\n",
    "      graph_df: A Pandas DataFrame of edges.\n",
    "      !!! Important: graph_df should contain ONLY `child` & `parent` columns\n",
    "    Returns:\n",
    "      A Pandas DataFrame of negative edges.\n",
    "    \"\"\"\n",
    "    edges_set = set(map(tuple, graph_df.values)) # Set of all edges in the graph\n",
    "    non_edges = [] # Empty list to store the negative edges\n",
    "    pbar = tqdm(total=len(graph_df))# Progress bar\n",
    "    while len(non_edges) < len(graph_df):\n",
    "        sample_size = min(2 ** 17, len(graph_df) - len(non_edges)) # 2 ** 17 overflow\n",
    "        # Randomly sample `sample_size` values from each column in the graph_df DataFrame\n",
    "        column_samples = [np.random.choice(graph_df[c], sample_size) for c in graph_df.columns]\n",
    "        # Create a list of tuples representing the negative edges\n",
    "        new_non_edges = [sample for sample in zip(*column_samples) if sample not in edges_set]\n",
    "        # Add the new negative edges to the list of negative edges\n",
    "        non_edges += new_non_edges\n",
    "        # Update the progress bar\n",
    "        pbar.update(len(new_non_edges))\n",
    "    \n",
    "    pbar.close()# Close progress bar\n",
    "    return pd.DataFrame(non_edges, columns=graph_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff60d39",
   "metadata": {},
   "source": [
    "### 0 Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fcbbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoliticalCompass:\n",
      "\t users loaded (unique users: 22503) ✓\n",
      "\t edges loaded (unique users: 19124) ✓\n",
      "\t socio-demographic features loaded (unique users: 18255) ✓\n",
      "\t socio-demographic features modified ✓\n",
      "\t confounding features loaded ✓\n",
      "\t confounding features selected ✓\n",
      "\t users popularity classified ✓\n",
      "PoliticalCompassMemes:\n",
      "\t users loaded (unique users: 258428) ✓\n",
      "\t edges loaded (unique users: 223304) ✓\n",
      "\t socio-demographic features loaded (unique users: 251344) ✓\n",
      "\t socio-demographic features modified ✓\n",
      "\t confounding features loaded ✓\n",
      "\t confounding features selected ✓\n",
      "\t users popularity classified ✓\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "# Users and edges of interaction networks, users socio-demographic features\n",
    "users_df = {}\n",
    "edges_df = {}\n",
    "socio_demographic_features = {}\n",
    "confounding_features = {}\n",
    "\n",
    "for S in SUBREDDITS:\n",
    "    \n",
    "    # Users with single flair\n",
    "        # author, flair\n",
    "    users_df[S] = pd.read_csv(OUTPUT_PATH + f\"single_flair_anonymized_users_{SUBREDDITS[S]}.csv\")\n",
    "    print(f\"{S}:\")\n",
    "    print(f\"\\t users loaded (unique users: {len(users_df[S])}) ✓\")\n",
    "    \n",
    "    # Edges from Interaction Network\n",
    "        # child, parent\n",
    "    edges_df[S] = pd.read_csv(DATA_PATH + f\"edges_anonymized_{SUBREDDITS[S]}.csv\")\n",
    "    \n",
    "    edges_df[S] = edges_df[S][['child', 'parent']]\n",
    "    print(f\"\\t edges loaded (unique users: {len(set(edges_df[S]['child'].unique()).union(set(edges_df[S]['parent'].unique())))}) ✓\")\n",
    "    \n",
    "    # Users scores for socio-demographic features\n",
    "        # 'user', 'age', 'gender', 'affluence', 'social', 'economic'\n",
    "    socio_demographic_features[S] = (pd.read_csv(DATA_PATH + f\"socio_demographics_anonymized_{SUBREDDITS[S]}.csv\"))\n",
    "    print(f\"\\t socio-demographic features loaded (unique users: {len(set(socio_demographic_features[S]['user'].unique()))}) ✓\")\n",
    "    # Remove not usefull columns (partisan, flair)\n",
    "    socio_demographic_features[S] = (socio_demographic_features[S].drop(columns=['partisan', 'flair']))\n",
    "    # Modify values for social/economic Centrist: Centrist -> Centrist_social/Centrist_economic\n",
    "    for axis in ['social', 'economic']:\n",
    "        socio_demographic_features[S][axis] = (socio_demographic_features[S]\n",
    "                                               .apply(lambda x: x[axis] + f'_{axis}'\n",
    "                                                      if x[axis] == 'Centrist' else x[axis],\n",
    "                                                      axis=1\n",
    "                                                     )\n",
    "                                              )\n",
    "    # Bool values for socio-demographic\n",
    "    for feature, (class_low, class_hi) in SOCIO_DEM2CLASSES.items():\n",
    "            socio_demographic_features[S][class_low] = socio_demographic_features[S][feature] <= 0.25\n",
    "            socio_demographic_features[S][class_hi] = socio_demographic_features[S][feature] >= 0.75\n",
    "            del socio_demographic_features[S][feature]\n",
    "    print(f\"\\t socio-demographic features modified ✓\")\n",
    "    \n",
    "    # Confounding features:\n",
    "        # 'author', 'flair', 'num_comm', 'avg_score', 'num_comm_pos', 'frac_pos'\n",
    "    confounding_features[S] = pd.read_csv(OUTPUT_PATH + f\"popularity_metrics_anonymized_{SUBREDDITS[S]}.csv\")\n",
    "    print(f\"\\t confounding features loaded ✓\")\n",
    "\n",
    "    # Select comments number, average score and positive score comments number\n",
    "    confounding_features[S] = (users_df[S].join(confounding_features[S][['author', 'avg_score']].set_index('author'), on='author'))\n",
    "    print(f\"\\t confounding features selected ✓\")\n",
    "    \n",
    "    #confounding_features[S]['frac_pos'] = confounding_features[S]['num_comm_pos'] / confounding_features[S]['num_comm']\n",
    "    #confounding_features[S] = confounding_features[S].dropna()\n",
    "    confounding_features[S]['avg_score_q'] = quantile_normalize(confounding_features[S]['avg_score'])\n",
    "    confounding_features[S]['popularity'] = pd.qcut(\n",
    "        confounding_features[S]['avg_score_q'],\n",
    "        q=len(POPULARITY_BIN),\n",
    "        labels=POPULARITY_BIN, retbins=True)[0]\n",
    "    confounding_features[S] = confounding_features[S].dropna()\n",
    "    print(f\"\\t users popularity classified ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6442b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    return create_political_compass_features(df) + create_socio_attrib_features(df) + create_popularity_features(df)\n",
    "\n",
    "def create_popularity_features(df):\n",
    "    return [(\"Target_is_popular\", df['s4_parent']), (\"Target_is_not_popular\", df['s1_parent'])]\n",
    "\n",
    "def create_socio_attrib_features(df):\n",
    "    import itertools\n",
    "    socio_attrib_combinations = list(itertools.combinations(SOCIO_DEM_CLASSES, 2)) + [(a, a) for a in SOCIO_DEM_CLASSES]\n",
    "    return [(\n",
    "                f\"Symmetric_{a1}_{a2}\", (df[f'{a1}_child'] & df[f'{a2}_parent']) | (df[f'{a2}_child'] & df[f'{a1}_parent'])                           \n",
    "            ) \n",
    "            for a1, a2 in socio_attrib_combinations\n",
    "    ]\n",
    "\n",
    "def create_political_compass_features(df):\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            \"Homo_Economic\", (\n",
    "                            (df['Left_child'] & df['Left_parent']) | \n",
    "                            (df['Right_child'] & df['Right_parent'])\n",
    "                            )\n",
    "        ),\n",
    "        (\n",
    "            \"Hete_Economic\", (\n",
    "                            (df['Left_child'] & df['Right_parent']) | \n",
    "                            (df['Right_child'] & df['Left_parent'])\n",
    "                            )\n",
    "        ),\n",
    "        (\n",
    "            \"Hete_Economic_Source_Left\", (\n",
    "                            (df['Left_child'] & df['Right_parent'])\n",
    "                            )\n",
    "        ),\n",
    "        (\n",
    "            \"Homo_Social\", (\n",
    "                            (df['Lib_child'] & df['Lib_parent']) | \n",
    "                            (df['Auth_child'] & df['Auth_parent'])\n",
    "                            )\n",
    "        ),\n",
    "        (\n",
    "            \"Hete_Social\", (\n",
    "                            (df['Lib_child'] & df['Auth_parent']) | \n",
    "                            (df['Auth_child'] & df['Lib_parent'])\n",
    "                            )\n",
    "        ),\n",
    "        (\n",
    "            \"Hete_Social_Source_Lib\", (\n",
    "                            (df['Lib_child'] & df['Auth_parent'])\n",
    "                            )\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426ac27",
   "metadata": {},
   "source": [
    "# Build X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcfeab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261078/261078 [00:00<00:00, 2999059.23it/s]\n",
      "/tmp/ipykernel_1232109/3602297826.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  edges[S]['is_link'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoliticalCompass:\n",
      "\t sampled negative edges ✓\n",
      "\t get ideologies for child, parent ✓\n",
      "\t get confounging & socio-demographic features ✓\n",
      "\t get popularity classes for child, parent\n",
      "\t get Y (is_link: 1), X (ideologies: 12, socio-dem: 12, confounding: 8) ✓\n",
      "\t shuffled edges ✓\n",
      "\t get variables names ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 4130.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t get X ✓\n",
      "\t get Y ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           30     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.46806D-03\n",
      "\n",
      "At iterate    1    f=  6.92188D-01    |proj g|=  1.18237D-03\n",
      "\n",
      "At iterate    2    f=  6.92091D-01    |proj g|=  9.06011D-04\n",
      "\n",
      "At iterate    3    f=  6.92005D-01    |proj g|=  1.10593D-03\n",
      "\n",
      "At iterate    4    f=  6.91998D-01    |proj g|=  6.18178D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    5    f=  6.91992D-01    |proj g|=  4.58466D-04\n",
      "\n",
      "At iterate    6    f=  6.91973D-01    |proj g|=  2.41686D-04\n",
      "\n",
      "At iterate    7    f=  6.91965D-01    |proj g|=  1.40055D-04\n",
      "\n",
      "At iterate    8    f=  6.91963D-01    |proj g|=  9.37923D-05\n",
      "\n",
      "At iterate    9    f=  6.91963D-01    |proj g|=  3.70209D-05\n",
      "\n",
      "At iterate   10    f=  6.91963D-01    |proj g|=  1.94937D-05\n",
      "\n",
      "At iterate   11    f=  6.91963D-01    |proj g|=  8.84677D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   30     11     14      1     0     0   8.847D-06   6.920D-01\n",
      "  F =  0.69196259417461770     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8065395/8065395 [00:03<00:00, 2440775.12it/s]\n",
      "/tmp/ipykernel_1232109/3602297826.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  edges[S]['is_link'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoliticalCompassMemes:\n",
      "\t sampled negative edges ✓\n",
      "\t get ideologies for child, parent ✓\n",
      "\t get confounging & socio-demographic features ✓\n",
      "\t get popularity classes for child, parent\n",
      "\t get Y (is_link: 1), X (ideologies: 12, socio-dem: 12, confounding: 8) ✓\n",
      "\t shuffled edges ✓\n",
      "\t get variables names ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 123.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t get X ✓\n",
      "\t get Y ✓\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           30     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.04355D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  6.91243D-01    |proj g|=  6.09800D-03\n",
      "\n",
      "At iterate    2    f=  6.91151D-01    |proj g|=  1.09544D-03\n",
      "\n",
      "At iterate    3    f=  6.91063D-01    |proj g|=  9.73142D-04\n",
      "\n",
      "At iterate    4    f=  6.90994D-01    |proj g|=  4.34950D-04\n",
      "\n",
      "At iterate    5    f=  6.90969D-01    |proj g|=  4.21634D-04\n",
      "\n",
      "At iterate    6    f=  6.90961D-01    |proj g|=  4.47200D-04\n",
      "\n",
      "At iterate    7    f=  6.90953D-01    |proj g|=  1.00905D-04\n",
      "\n",
      "At iterate    8    f=  6.90952D-01    |proj g|=  4.12017D-05\n",
      "\n",
      "At iterate    9    f=  6.90952D-01    |proj g|=  3.98874D-05\n",
      "\n",
      "At iterate   10    f=  6.90951D-01    |proj g|=  2.25501D-05\n",
      "\n",
      "At iterate   11    f=  6.90951D-01    |proj g|=  1.78145D-05\n",
      "\n",
      "At iterate   12    f=  6.90951D-01    |proj g|=  1.47698D-05\n",
      "\n",
      "At iterate   13    f=  6.90951D-01    |proj g|=  2.08871D-05\n",
      "\n",
      "At iterate   14    f=  6.90951D-01    |proj g|=  2.19786D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   30     14     19      1     0     0   2.198D-06   6.910D-01\n",
      "  F =  0.69095103102167532     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "edges = {}\n",
    "non_edges = {}\n",
    "regression_df = {}\n",
    "regression_variables = {}\n",
    "variable_names = {}\n",
    "X = {}\n",
    "y = {}\n",
    "logreg = {}\n",
    "logreg_res = {}\n",
    "\n",
    "for S in SUBREDDITS:\n",
    "    np.random.seed(44)\n",
    "    \n",
    "    edges[S] = edges_df[S][(edges_df[S]['child'].isin(set(socio_demographic_features[S]['user'].unique()))) & \n",
    "                           (edges_df[S]['parent'].isin(set(socio_demographic_features[S]['user'].unique())))\n",
    "                          ]\n",
    "    non_edges[S] = subsample_neg_pairs(edges[S])\n",
    "\n",
    "    edges[S]['is_link'] = 1\n",
    "    non_edges[S]['is_link'] = 0\n",
    "    print(f\"{S}:\")\n",
    "    print(f\"\\t sampled negative edges ✓\")\n",
    "    \n",
    "    regression_df[S] = ideologic_features(pd.concat([edges[S], non_edges[S]]), users_df[S][['author', 'flair']])\n",
    "        #child, parent, is_link, Left_child, Left_parent, Centrist_economic_child, Centrist_economic_parent,\n",
    "        #Right_child, Right_parent, Lib_child, Lib_parent, Centrist_social_child, Centrist_social_parent,\n",
    "        #Auth_child, Auth_parent\n",
    "    for feature, classes in IDEOLOGIES2CLASSES.items():\n",
    "        for c in classes:\n",
    "            regression_df[S][f'{c}_child'] = regression_df[S][f'{feature}_child'] == c\n",
    "            regression_df[S][f'{c}_parent'] = regression_df[S][f'{feature}_parent'] == c\n",
    "        del regression_df[S][f'{feature}_child']\n",
    "        del regression_df[S][f'{feature}_parent']\n",
    "    print(f\"\\t get ideologies for child, parent ✓\")\n",
    "    \n",
    "    regression_df[S] = (regression_df[S]\n",
    "                        # Popularity features (child, parent)\n",
    "                         .merge(confounding_features[S][['author', 'popularity']].set_index('author'),\n",
    "                                right_on='author', left_on='child')\n",
    "                        .merge(confounding_features[S][['author', 'popularity']].set_index('author'),\n",
    "                               right_on='author', left_on='parent', suffixes=('_child', '_parent'))\n",
    "                        # Socio-demographic features (child, parent)\n",
    "                        .merge((socio_demographic_features[S]\n",
    "                                [['user', 'Young', 'Old', 'Male', 'Female', 'Poor', 'Rich']]\n",
    "                                .set_index('user')), left_on='child', right_on='user')\n",
    "                        .merge((socio_demographic_features[S]\n",
    "                                [['user', 'Young', 'Old', 'Male', 'Female', 'Poor', 'Rich']]\n",
    "                                .set_index('user')),\n",
    "                               left_on='parent', right_on='user', suffixes=('_child', '_parent'))\n",
    "                       )\n",
    "    print(f\"\\t get confounging & socio-demographic features ✓\")\n",
    "    \n",
    "    for popularity in POPULARITY_BIN:\n",
    "        regression_df[S][f'{popularity}_child'] = regression_df[S]['popularity_child'] == popularity\n",
    "        regression_df[S][f'{popularity}_parent'] = regression_df[S]['popularity_parent'] == popularity\n",
    "    del regression_df[S]['popularity_child']\n",
    "    del regression_df[S]['popularity_parent']\n",
    "    print(f\"\\t get popularity classes for child, parent\")\n",
    "        \n",
    "    regression_df[S] = regression_df[S][['is_link', # Y\n",
    "                                         # X\n",
    "                                             # ideologies\n",
    "                                         'Left_child', 'Left_parent', #'Centrist_economic_child',\n",
    "                                         'Right_child', 'Right_parent', 'Lib_child', # 'Centrist_economic_parent', \n",
    "                                         'Lib_parent', #'Centrist_social_child', #'Centrist_social_parent',\n",
    "                                         'Auth_child', 'Auth_parent',\n",
    "                                             # socio-demographic\n",
    "                                         'Young_child', 'Old_child', 'Male_child', 'Female_child', 'Poor_child',\n",
    "                                         'Rich_child', 'Young_parent', 'Old_parent', 'Male_parent',\n",
    "                                         'Female_parent', 'Poor_parent', 'Rich_parent',\n",
    "                                             # popularity\n",
    "                                         's1_child', 's1_parent', 's2_child','s2_parent',\n",
    "                                         's3_child', 's3_parent', 's4_child', 's4_parent'\n",
    "                                        ]]\n",
    "    print(f\"\\t get Y (is_link: 1), X (ideologies: 12, socio-dem: 12, confounding: 8) ✓\")\n",
    "    \n",
    "    regression_df[S] = sklearn.utils.shuffle(regression_df[S])\n",
    "    print(f\"\\t shuffled edges ✓\")    \n",
    "    \n",
    "    regression_variables[S] = create_features(regression_df[S])\n",
    "    variable_names[S] = [name for name, value in regression_variables[S]]\n",
    "    print(f\"\\t get variables names ✓\")\n",
    "\n",
    "    # Compute X\n",
    "    batch_size = 1\n",
    "\n",
    "    # Calculate the total number of rows that will be in the final array\n",
    "    total_rows = len(regression_variables[S])\n",
    "\n",
    "    # Calculate the number of columns based on one of the arrays\n",
    "    num_columns = len(regression_variables[S][0][1])\n",
    "\n",
    "    # Preallocate the array\n",
    "    X[S] = np.empty((total_rows, num_columns), dtype=np.int8)\n",
    "\n",
    "    # Fill the preallocated array in batches\n",
    "    for i in tqdm(range(0, total_rows, batch_size)):\n",
    "        batch = regression_variables[S][i:i + batch_size]\n",
    "        stacked = np.vstack([value for name, value in batch]).astype(np.int8)\n",
    "        X[S][i:i + batch_size] = stacked  # Fill in place\n",
    "\n",
    "    X[S] = X[S].T\n",
    "\n",
    "    print(f\"\\t get X ✓\")\n",
    "\n",
    "    y[S] = np.array(regression_df[S].is_link.values)\n",
    "    print(f\"\\t get Y ✓\")\n",
    "\n",
    "    logreg[S] = sm.Logit(y[S], sm.add_constant(X[S], has_constant='add'), missing='raise')\n",
    "    logreg_res[S] = logreg[S].fit(maxiter=1000, disp=True, method='lbfgs') #'lbfgs'\n",
    "    \n",
    "    # Extract the relevant data\n",
    "    coefficients = logreg_res[S].params[1:]  # skip the constant\n",
    "    conf_int = logreg_res[S].conf_int(alpha=0.05)[1:]  # 95% CI\n",
    "    pvalues = logreg_res[S].pvalues[1:]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'feature_name': variable_names[S],\n",
    "        'coefficient': coefficients,\n",
    "        'pvalue': pvalues,\n",
    "        'conf_int_lower': conf_int[:, 0],\n",
    "        'conf_int_upper': conf_int[:, 1]\n",
    "    })\n",
    "\n",
    "    data.to_csv(OUTPUT_PATH + f\"features_importance_{SUBREDDITS[S]}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
